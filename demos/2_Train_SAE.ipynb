{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6357a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xgao/anaconda3/envs/sae/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vit_prisma.sae import VisionModelSAERunnerConfig\n",
    "from vit_prisma.sae import VisionSAETrainer\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e1ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HTTP_PROXY'] = 'http://127.0.0.1:17890'\n",
    "# os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:17890'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac9851",
   "metadata": {},
   "source": [
    "# Train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7e3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:21:53 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-06-22 05:21:54 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/resolve/main/weights.pt HTTP/1.1\" 302 0\n",
      "2025-06-22 05:21:54 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /data/xgao/.cache/huggingface/hub/models--Prisma-Multimodal--sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/snapshots/b46d6e9d6c114a53364c5c172ea5023e0e52d4e2/weights.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:21:55 WARNING:root: Deprecated field 'total_training_images' found in config. It will be ignored.\n",
      "2025-06-22 05:21:55 WARNING:root: Deprecated field 'total_training_tokens' found in config. It will be ignored.\n",
      "2025-06-22 05:21:55 WARNING:root: Deprecated field 'd_sae' found in config. It will be ignored.\n",
      "2025-06-22 05:21:55 INFO:root: n_tokens_per_buffer (millions): 0.032\n",
      "2025-06-22 05:21:55 INFO:root: Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "2025-06-22 05:21:55 INFO:root: Total training steps: 158691\n",
      "2025-06-22 05:21:55 INFO:root: Total training images: 13000000\n",
      "2025-06-22 05:21:55 INFO:root: Total wandb updates: 1586\n",
      "2025-06-22 05:21:55 INFO:root: Expansion factor: 64\n",
      "2025-06-22 05:21:55 INFO:root: n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "2025-06-22 05:21:55 INFO:root: n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "2025-06-22 05:21:55 INFO:root: We will reset the sparsity calculation 158 times.\n",
      "2025-06-22 05:21:55 INFO:root: Number tokens in sparsity calculation window: 4.10e+06\n",
      "2025-06-22 05:21:55 INFO:root: Using Ghost Grads.\n",
      "2025-06-22 05:21:55 INFO:root: Gradient clipping with max_norm=1.0\n",
      "2025-06-22 05:21:55 INFO:root: Using SAE initialization method: encoder_transpose_decoder\n",
      "2025-06-22 05:21:55 INFO:root: get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load an SAE\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "\n",
    "def load_sae(repo_id, file_name, config_name):\n",
    "    # Step 1: Download SAE weights and SAE config from Hugginface\n",
    "    sae_path = hf_hub_download(repo_id, file_name) # Download SAE weights\n",
    "    hf_hub_download(repo_id, config_name) # Download SAE config\n",
    "\n",
    "    # Step 2: Now load the pretrained SAE weights from where you just downloaded them\n",
    "    print(f\"Loading SAE from {sae_path}...\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path) # This now automatically gets the config.json file in that folder and converts it into a VisionSAERunnerConfig object\n",
    "    return sae\n",
    "\n",
    "# Change the repo_id to the Huggingface repo of your chosen SAE. See /docs for a list of SAEs.\n",
    "repo_id = \"Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05\" \n",
    "\n",
    "file_name = \"weights.pt\" # Usually weights.pt but may have slight naming variation. See the chosen HF repo for the exact file name\n",
    "config_name = \"config.json\"\n",
    "\n",
    "sae = load_sae(repo_id, file_name, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2584867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:21:55 INFO:root: Model 'open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K' is supported and passes tests.\n",
      "2025-06-22 05:21:55 INFO:root: model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "2025-06-22 05:21:55 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_config.json HTTP/1.1\" 200 0\n",
      "2025-06-22 05:21:56 INFO:root: model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "2025-06-22 05:21:56 DEBUG:urllib3.connectionpool: https://huggingface.co:443 \"HEAD /laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K/resolve/main/open_clip_pytorch_model.bin HTTP/1.1\" 302 0\n",
      "2025-06-22 05:21:56 INFO:root: visual projection shape: torch.Size([768, 512])\n",
      "2025-06-22 05:21:57 INFO:root: Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedViT(\n",
       "  (embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  )\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbedding()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (hook_full_embed): HookPoint()\n",
       "  (ln_pre): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_pre): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_final): HookPoint()\n",
       "  (head): Head()\n",
       "  (hook_post_head_pre_normalize): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = sae.cfg.model_name\n",
    "model = load_hooked_model(model_name)\n",
    "model.to(DEVICE) # Move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d885360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.63s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Put your ImageNet Paths here\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "# imagenet_train_path = '/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train'\n",
    "# imagenet_validation_path = '/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val'\n",
    "\n",
    "# data_transforms = get_clip_val_transforms()\n",
    "# train_dataset = torchvision.datasets.ImageFolder(imagenet_train_path, transform=data_transforms)\n",
    "# eval_dataset = torchvision.datasets.ImageFolder(imagenet_validation_path, transform=data_transforms)\n",
    "\n",
    "mscoco_root = '/data/wyx/datasets/COCO'\n",
    "mscoco_train_img_dir = f'{mscoco_root}/train2017'\n",
    "mscoco_val_img_dir = f'{mscoco_root}/val2017'\n",
    "mscoco_train_ann_file = f'{mscoco_root}/annotations/instances_train2017.json'\n",
    "mscoco_val_ann_file = f'{mscoco_root}/annotations/instances_val2017.json'\n",
    "\n",
    "data_transforms = get_clip_val_transforms()\n",
    "train_dataset = CocoDetection(mscoco_train_img_dir, mscoco_train_ann_file, transform=data_transforms)\n",
    "eval_dataset = CocoDetection(mscoco_val_img_dir, mscoco_val_ann_file, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f0fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:22:16 INFO:root: n_tokens_per_buffer (millions): 0.032\n",
      "2025-06-22 05:22:16 INFO:root: Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "2025-06-22 05:22:16 INFO:root: Total training steps: 15869\n",
      "2025-06-22 05:22:16 INFO:root: Total training images: 1300000\n",
      "2025-06-22 05:22:16 INFO:root: Total wandb updates: 1586\n",
      "2025-06-22 05:22:16 INFO:root: Expansion factor: 16\n",
      "2025-06-22 05:22:16 INFO:root: n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "2025-06-22 05:22:16 INFO:root: n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "2025-06-22 05:22:16 INFO:root: We will reset the sparsity calculation 15 times.\n",
      "2025-06-22 05:22:16 INFO:root: Number tokens in sparsity calculation window: 4.10e+06\n",
      "2025-06-22 05:22:16 INFO:root: Gradient clipping with max_norm=1.0\n",
      "2025-06-22 05:22:16 INFO:root: Using SAE initialization method: independent\n"
     ]
    }
   ],
   "source": [
    "sae_trainer_cfg = VisionModelSAERunnerConfig( \n",
    "    hook_point_layer=10,\n",
    "    layer_subtype='hook_resid_post',\n",
    "    dataset_name=\"mscoco\",\n",
    "    feature_sampling_window=1000,\n",
    "    activation_fn_str='relu',\n",
    "    checkpoint_path = '/data/xgao/code/interpretability/VIT-PRISMA/ckpts',\n",
    "    log_to_wandb = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e921b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:22:16 DEBUG:git.cmd: Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/data/xgao/code/interpretability/ViT-Prisma, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-06-22 05:22:16 DEBUG:urllib3.connectionpool: Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2025-06-22 05:22:16 DEBUG:urllib3.connectionpool: https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
      "2025-06-22 05:22:16 DEBUG:urllib3.connectionpool: https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgaox23\u001b[0m (\u001b[33mgaox23-fdu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2025-06-22 05:22:17 DEBUG:git.cmd: Popen(['git', 'cat-file', '--batch-check'], cwd=/data/xgao/code/interpretability/ViT-Prisma, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/xgao/code/interpretability/ViT-Prisma/demos/wandb/run-20250622_052216-9dg6jmxj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaox23-fdu/test/runs/9dg6jmxj' target=\"_blank\">misty-cherry-3</a></strong> to <a href='https://wandb.ai/gaox23-fdu/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaox23-fdu/test' target=\"_blank\">https://wandb.ai/gaox23-fdu/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaox23-fdu/test/runs/9dg6jmxj' target=\"_blank\">https://wandb.ai/gaox23-fdu/test/runs/9dg6jmxj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gaox23-fdu/test/runs/9dg6jmxj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe7fe8f7400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"test\", mode=\"online\")  # 或 mode=\"online\" 测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6a4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset HTTP_PROXY\n",
    "!unset HTTPS_PROXY\n",
    "import os\n",
    "os.environ.pop(\"HTTP_PROXY\", None)\n",
    "os.environ.pop(\"HTTPS_PROXY\", None)\n",
    "!printenv HTTP_PROXY\n",
    "!printenv HTTPS_PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e7d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:22:19 INFO:root: get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:22:23 DEBUG:asyncio: Using selector: EpollSelector\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-cherry-3</strong> at: <a href='https://wandb.ai/gaox23-fdu/test/runs/9dg6jmxj' target=\"_blank\">https://wandb.ai/gaox23-fdu/test/runs/9dg6jmxj</a><br> View project at: <a href='https://wandb.ai/gaox23-fdu/test' target=\"_blank\">https://wandb.ai/gaox23-fdu/test</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250622_052216-9dg6jmxj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 05:22:23 DEBUG:git.cmd: Popen(['git', 'cat-file', '--batch-check'], cwd=/data/xgao/code/interpretability/ViT-Prisma, stdin=<valid stream>, shell=False, universal_newlines=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/xgao/code/interpretability/ViT-Prisma/demos/wandb/run-20250622_052222-a0nbdqh8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaox23-fdu/tinyclip_sae_16_hyperparam_sweep_lr/runs/a0nbdqh8' target=\"_blank\">267a5b83-tinyclip_sae_16_hyperparam_sweep_lr</a></strong> to <a href='https://wandb.ai/gaox23-fdu/tinyclip_sae_16_hyperparam_sweep_lr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaox23-fdu/tinyclip_sae_16_hyperparam_sweep_lr' target=\"_blank\">https://wandb.ai/gaox23-fdu/tinyclip_sae_16_hyperparam_sweep_lr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaox23-fdu/tinyclip_sae_16_hyperparam_sweep_lr/runs/a0nbdqh8' target=\"_blank\">https://wandb.ai/gaox23-fdu/tinyclip_sae_16_hyperparam_sweep_lr/runs/a0nbdqh8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 478317.9688:   1%|          | 2/200 [00:00<00:02, 85.45it/s]\n",
      "Training SAE: Loss: 0.0175, MSE Loss: 0.0136, L1 Loss: 0.0039, L0: 11.0593:   8%|▊         | 5181440/65000000 [03:24<39:41, 25119.77it/s]2025-06-22 05:25:54 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:25:54 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0174, MSE Loss: 0.0135, L1 Loss: 0.0039, L0: 10.8109:   9%|▉         | 6172672/65000000 [04:04<39:37, 24738.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_130007.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0170, MSE Loss: 0.0130, L1 Loss: 0.0040, L0: 10.3187:  13%|█▎        | 8761344/65000000 [05:45<36:45, 25500.77it/s]2025-06-22 05:28:24 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:28:24 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0166, MSE Loss: 0.0125, L1 Loss: 0.0041, L0: 10.4377:  20%|█▉        | 12853248/65000000 [08:27<34:29, 25193.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_260014.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0165, MSE Loss: 0.0123, L1 Loss: 0.0042, L0: 10.5396:  26%|██▌       | 16969728/65000000 [11:09<31:22, 25516.56it/s]2025-06-22 05:33:34 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:33:34 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0162, MSE Loss: 0.0121, L1 Loss: 0.0042, L0: 10.6509:  30%|██▉       | 19460096/65000000 [12:50<30:52, 24587.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_390021.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0162, MSE Loss: 0.0121, L1 Loss: 0.0042, L0: 10.6743:  31%|███       | 19976192/65000000 [13:10<30:05, 24931.46it/s]2025-06-22 05:35:38 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:35:38 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0160, MSE Loss: 0.0118, L1 Loss: 0.0042, L0: 10.9399:  39%|███▉      | 25522176/65000000 [16:52<26:29, 24839.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_520028.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0159, MSE Loss: 0.0117, L1 Loss: 0.0042, L0: 11.2018:  46%|████▌     | 29618176/65000000 [19:33<23:20, 25258.08it/s]2025-06-22 05:42:04 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:42:04 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0159, MSE Loss: 0.0116, L1 Loss: 0.0043, L0: 11.2759:  49%|████▉     | 32157696/65000000 [21:14<21:39, 25266.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_650035.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0159, MSE Loss: 0.0117, L1 Loss: 0.0043, L0: 11.3419:  51%|█████     | 33157120/65000000 [21:55<21:19, 24890.06it/s]2025-06-22 05:44:27 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:44:27 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0158, MSE Loss: 0.0115, L1 Loss: 0.0043, L0: 11.3346:  60%|█████▉    | 38989824/65000000 [25:37<16:51, 25720.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_780042.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0157, MSE Loss: 0.0114, L1 Loss: 0.0043, L0: 11.4722:  64%|██████▎   | 41398272/65000000 [27:11<15:17, 25725.50it/s]2025-06-22 05:49:42 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:49:42 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0157, MSE Loss: 0.0114, L1 Loss: 0.0043, L0: 11.3532:  66%|██████▌   | 43036672/65000000 [28:19<14:54, 24545.46it/s]2025-06-22 05:50:46 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:50:46 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0158, MSE Loss: 0.0115, L1 Loss: 0.0043, L0: 11.5076:  70%|██████▉   | 45400064/65000000 [29:51<12:50, 25446.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_910049.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0157, MSE Loss: 0.0114, L1 Loss: 0.0043, L0: 11.6716:  80%|███████▉  | 51748864/65000000 [34:01<08:35, 25723.55it/s]2025-06-22 05:56:27 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 05:56:27 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0156, MSE Loss: 0.0113, L1 Loss: 0.0043, L0: 11.5800:  80%|███████▉  | 51769344/65000000 [34:02<08:32, 25839.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_1040056.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE: Loss: 0.0156, MSE Loss: 0.0113, L1 Loss: 0.0043, L0: 11.7209:  90%|████████▉ | 58421248/65000000 [38:24<04:16, 25627.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_1170063.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 06:01:05 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 06:01:05 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0156, MSE Loss: 0.0113, L1 Loss: 0.0043, L0: 11.6630:  99%|█████████▉| 64462848/65000000 [42:21<00:20, 25697.27it/s]2025-06-22 06:04:52 DEBUG:PIL.PngImagePlugin: STREAM b'IHDR' 16 13\n",
      "2025-06-22 06:04:52 DEBUG:PIL.PngImagePlugin: STREAM b'IDAT' 41 32768\n",
      "Training SAE: Loss: 0.0156, MSE Loss: 0.0113, L1 Loss: 0.0043, L0: 11.7430: : 65003520it [42:42, 25365.47it/s]                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SAE to /data/xgao/code/interpretability/VIT-PRISMA/ckpts/267a5b83-tinyclip_sae_16_hyperparam_sweep_lr/n_images_1300070.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "trainer = VisionSAETrainer(sae_trainer_cfg, model, train_dataset, eval_dataset)\n",
    "sae = trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71661941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
