from dataclasses import dataclass, field, fields, asdict
from typing import Optional, Literal, Any
import torch
import json
import os
import inspect
import logging
from vit_prisma.configs.HookedViTConfig import HookedViTConfig


# ----------------------
# Utility for dtype parsing
# ----------------------
dtype_mapping = {
    "float32": torch.float32,
    "float": torch.float32,
    "float64": torch.float64,
    "double": torch.float64,
    "float16": torch.float16,
    "half": torch.float16,
    "int64": torch.int64,
    "long": torch.int64,
    "int32": torch.int32,
    "int": torch.int32,
    "int16": torch.int16,
    "short": torch.int16,
    "int8": torch.int8,
    "uint8": torch.uint8,
    "bool": torch.bool,
    "torch.float32": torch.float32,
    "torch.float": torch.float32,
    "torch.float64": torch.float64,
    "torch.double": torch.float64,
    "torch.float16": torch.float16,
    "torch.half": torch.float16,
    "torch.int64": torch.int64,
    "torch.long": torch.int64,
    "torch.int32": torch.int32,
    "torch.int": torch.int32,
    "torch.int16": torch.int16,
    "torch.short": torch.int16,
    "torch.int8": torch.int8,
    "torch.uint8": torch.uint8,
    "torch.bool": torch.bool,
}


# @dataclass
# class BaseSAEConfig:
#     def save_config(self, path: str):
#         data = asdict(self)

#         def make_serializable(obj):
#             if inspect.isdatadescriptor(obj):
#                 return
#             if isinstance(obj, (list, tuple)):
#                 return [make_serializable(item) for item in obj]
#             elif isinstance(obj, dict):
#                 return {key: make_serializable(value) for key, value in obj.items()}
#             else:
#                 return obj

#         serializable_data = make_serializable(data)
#         if hasattr(self, "_dtype"):
#             serializable_data["_dtype"] = getattr(self, "_dtype")
#         if hasattr(self, "_device"):
#             serializable_data["_device"] = getattr(self, "_device")

#         with open(path, "w") as f:
#             json.dump(serializable_data, f, indent=4)

#     @classmethod
#     def load_config(cls, path: str):
#         with open(path, "r") as f:
#             data = json.load(f)

#         def reconstruct_types(obj):
#             if isinstance(obj, dict):
#                 return {key: reconstruct_types(value) for key, value in obj.items()}
#             elif isinstance(obj, list):
#                 return [reconstruct_types(item) for item in obj]
#             else:
#                 return obj

#         data = reconstruct_types(data)
#         current_fields = {f.name for f in fields(cls)}
#         for legacy_key in ["total_training_images", "total_training_tokens", "d_sae"]:
#             data.pop(legacy_key, None)
#         cleaned_data = {k: v for k, v in data.items() if k in current_fields}
#         return cls(**cleaned_data)

#     def pretty_print(self):
#         print(f"{self.__class__.__name__}:")
#         for field_ in fields(self):
#             value = getattr(self, field_.name)
#             if isinstance(value, torch.dtype):
#                 value = str(value).split(".")[-1]
#             elif isinstance(value, torch.device):
#                 value = str(value)
#             print(f"  {field_.name}: {value}")


# @dataclass
# class SAERunnerModelConfig(BaseSAEConfig):
#     model_class_name: str = "HookedViT"
#     model_name: str = "open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K"
#     vit_model_cfg: Optional[HookedViTConfig] = None
#     model_path: Optional[str] = None
#     hook_point_layer: int = 9
#     layer_subtype: str = "hook_resid_post"
#     hook_point_head_index: Optional[int] = None
#     context_size: int = 50
#     use_cached_activations: bool = False
#     use_patches_only: bool = False
#     cached_activations_path: Optional[str] = None
#     image_size: int = 224
#     architecture: Literal["standard", "gated", "jumprelu"] = "standard"

#     b_dec_init_method: str = "geometric_median"
#     expansion_factor: int = 16
#     from_pretrained_path: Optional[str] = None
#     d_in: int = 768
#     activation_fn_str: str = "topk"
#     activation_fn_kwargs: dict = field(default_factory=dict)
#     cls_token_only: bool = False
#     initialization_method: str = "independent"
#     normalize_activations: str = "layer_norm"

#     is_transcoder: bool = False
#     transcoder_with_skip_connection: bool = True
#     out_hook_point_layer: int = 9
#     layer_out_subtype: str = "hook_mlp_out"
#     d_out: int = 768

#     @property
#     def d_sae(self):
#         return self.d_in * self.expansion_factor

#     @property
#     def hook_point(self):
#         return f"blocks.{self.hook_point_layer}.{self.layer_subtype}"

#     @property
#     def out_hook_point(self):
#         return f"blocks.{self.out_hook_point_layer}.{self.layer_out_subtype}"

#     @property
#     def num_patch(self):
#         return int((self.context_size - 1) ** 0.5)


# @dataclass
# class SAERunnerTrainingConfig(BaseSAEConfig):
#     _device: str = "cuda"
#     _dtype: str = "float32"
#     seed: int = 42

#     dataset_name: str = "imagnet"
#     dataset_path: str = "/network/scratch/s/sonia.joseph/datasets/kaggle_datasets"
#     dataset_train_path: str = (
#         "/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train"
#     )
#     dataset_val_path: str = (
#         "/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val"
#     )

#     is_training: bool = True
#     num_epochs: int = 1
#     train_batch_size: int = 1024 * 4
#     max_grad_norm: float = 1.0

#     verbose: bool = False
#     log_to_wandb: bool = True
#     wandb_project: str = "tinyclip_sae_16_hyperparam_sweep_lr"
#     wandb_entity: Optional[str] = None
#     wandb_log_frequency: int = 10

#     lr: float = 0.001
#     l1_coefficient: float = 0.0002
#     lp_norm: float = 1
#     lr_scheduler_name: str = "cosineannealingwarmup"
#     lr_warm_up_steps: int = 500

#     n_validation_runs: int = 0
#     n_checkpoints: int = 10
#     checkpoint_path: str = (
#         "/network/scratch/p/praneet.suresh/open_clip_celeba_checkpoints/"
#     )

#     n_batches_in_buffer: int = 20
#     store_batch_size: int = 32
#     num_workers: int = 16

#     use_ghost_grads: bool = False
#     feature_sampling_window: int = 1000
#     dead_feature_window: int = 5000
#     dead_feature_threshold: float = 1e-8

#     min_l0: Optional[float] = None
#     min_explained_variance: Optional[float] = None

#     @property
#     def device(self):
#         return torch.device(self._device)

#     @device.setter
#     def device(self, value: str):
#         self._device = value

#     @property
#     def dtype(self):
#         return dtype_mapping[self._dtype]

#     @dtype.setter
#     def dtype(self, value: str):
#         self._dtype = value

#     @property
#     def total_training_images(self):
#         return int(1_300_000 * self.num_epochs)

#     @property
#     def total_training_tokens(self):
#         if self.cls_token_only:
#             tokens_per_image = 1
#         elif self.use_patches_only:
#             tokens_per_image = self.context_size - 1
#         else:
#             tokens_per_image = self.context_size
#         return self.total_training_images * tokens_per_image

#     @property
#     def total_training_steps(self):
#         return self.total_training_tokens // self.train_batch_size


# import json
# import math
# from abc import ABC
# from dataclasses import fields, field, asdict, dataclass
# from typing import Any, Optional, Literal
# import logging
# import inspect

# import os

# import torch
# from vit_prisma.configs.HookedViTConfig import HookedViTConfig

# # Define a mapping from string to torch.dtype
# dtype_mapping = {
#     "float32": torch.float32,
#     "float": torch.float32,  # alias
#     "float64": torch.float64,
#     "double": torch.float64,  # alias
#     "float16": torch.float16,
#     "half": torch.float16,  # alias
#     "int64": torch.int64,
#     "long": torch.int64,  # alias
#     "int32": torch.int32,
#     "int": torch.int32,  # alias
#     "int16": torch.int16,
#     "short": torch.int16,  # alias
#     "int8": torch.int8,
#     "uint8": torch.uint8,
#     "bool": torch.bool,
#     "torch.float32": torch.float32,
#     "torch.float": torch.float32,  # alias
#     "torch.float64": torch.float64,
#     "torch.double": torch.float64,  # alias
#     "torch.float16": torch.float16,
#     "torch.half": torch.float16,  # alias
#     "torch.int64": torch.int64,
#     "torch.long": torch.int64,  # alias
#     "torch.int32": torch.int32,
#     "torch.int": torch.int32,  # alias
#     "torch.int16": torch.int16,
#     "torch.short": torch.int16,  # alias
#     "torch.int8": torch.int8,
#     "torch.uint8": torch.uint8,
#     "torch.bool": torch.bool,
# }

from vit_prisma.configs.HookedViTConfig import HookedViTConfig


@dataclass
class VisionModelSAERunnerConfig:
    """
    Configuration for training a sparse autoencoder on a vision model.
    """

    ##################
    ### SAE Setup: ###
    ##################

    # Data Generating Function (Model + Training Distibuion)
    model_class_name: str = "HookedViT"
    model_name: str = "open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K"
    vit_model_cfg: Optional[HookedViTConfig] = None
    model_path: str = None
    hook_point_layer: int = 9
    layer_subtype: str = "ln2.hook_normalized"
    hook_point_head_index: Optional[int] = None
    context_size: int = 50
    use_cached_activations: bool = False
    use_patches_only: bool = False
    cached_activations_path: Optional[str] = (
        None  # Defaults to "activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}"
    )
    image_size: int = 224
    architecture: Literal["standard", "gated", "jumprelu"] = "standard"

    # SAE Parameters
    b_dec_init_method: str = "geometric_median"
    expansion_factor: int = 16
    from_pretrained_path: Optional[str] = None

    # Transcoder Parameters
    is_transcoder: bool = False
    transcoder_with_skip_connection: bool = True
    out_hook_point_layer: int = 9
    layer_out_subtype: str = "hook_mlp_out"
    d_out: int = 768

    # Misc
    _device: str = "cuda"
    seed: int = 42
    _dtype: str = "float32"

    # SAE Parameters
    d_in: int = 768
    activation_fn_str: str = "topk"  # relu or topk
    activation_fn_kwargs: dict = field(default_factory=dict)
    cls_token_only: bool = False  # use only CLS token in training

    # New changes
    max_grad_norm: float = 1.0  # For gradient clipping, set to None to turn off
    initialization_method: str = (
        "independent"  # "encoder_transpose_decoder"  # or independent
    )
    normalize_activations: str = "layer_norm"

    #####################
    ### SAE Training: ###
    #####################

    is_training = True

    # Activation Store Parameters
    n_batches_in_buffer: int = 20
    store_batch_size: int = 32
    num_workers: int = 16

    # Training length parameters
    num_epochs: int = 1

    # Logging
    verbose: bool = False

    # Training Parameters
    l1_coefficient: float = 0.0002  # 0.00008
    lp_norm: float = 1
    lr: float = 0.001
    lr_scheduler_name: str = (
        "cosineannealingwarmup"  # constant, constantwithwarmup, linearwarmupdecay, cosineannealing, cosineannealingwarmup
    )
    lr_warm_up_steps: int = 500

    train_batch_size: int = 1024 * 4

    # SAE Training run tolerance
    min_l0 = None
    min_explained_variance = None

    # Imagenet1k
    dataset_name: str = "imgnet"
    dataset_path: str = "/network/scratch/s/sonia.joseph/datasets/kaggle_datasets"
    dataset_train_path: str = (
        "/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train"
    )
    dataset_val_path: str = (
        "/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val"
    )

    # Resampling protocol args
    use_ghost_grads: bool = False
    feature_sampling_window: int = 1000  # 1000
    dead_feature_window: int = 5000  # unless this window is larger feature sampling,

    dead_feature_threshold: float = 1e-8

    # WANDB
    log_to_wandb: bool = True
    wandb_project: str = "tinyclip_sae_16_hyperparam_sweep_lr"
    wandb_entity: Optional[str] = None
    wandb_log_frequency: int = 10

    # Misc
    n_validation_runs: int = 0  # spaced linearly throughout training
    n_checkpoints: int = 10
    checkpoint_path: str = (
        "/network/scratch/p/praneet.suresh/open_clip_celeba_checkpoints/"
    )

    @property
    def device(self):
        """Device property, returns the torch device representation of the internal _device variable."""
        if isinstance(self._device, str):
            return torch.device(self._device)
        return self._device

    @device.setter
    def device(self, value: str):
        """Device setter to update the internal _device variable."""
        self._device = value

    @property
    def dtype(self):
        """Data type property for model precision."""
        return dtype_mapping[self._dtype]

    @dtype.setter
    def dtype(self, value: str):
        """Setter to update the data type property."""
        self._dtype = value

    @property
    def hook_point(self):
        """Returns the hook point identifier string for a specific layer."""
        return f"blocks.{self.hook_point_layer}.{self.layer_subtype}"

    @hook_point.setter
    def hook_point(self, value):
        # Store the custom hook point
        self._custom_hook_point = value

    @property
    def out_hook_point(self):
        """Returns the hook point identifier string for a specific layer."""
        return f"blocks.{self.out_hook_point_layer}.{self.layer_out_subtype}"

    @property
    def tokens_per_buffer(self):
        """Calculates the total number of tokens per buffer, considering context and batch settings."""
        if self.cls_token_only:  # Only use the CLS token per image
            tokens_per_image = 1
        elif self.use_patches_only:  # Exclude CLS token
            tokens_per_image = self.context_size - 1
        else:
            tokens_per_image = self.context_size

        return self.train_batch_size * tokens_per_image * self.n_batches_in_buffer

    @property
    def total_training_tokens(self):
        """Computes the total number of training tokens for the dataset and configuration."""
        if self.cls_token_only:  # Only use the CLS token per image
            tokens_per_image = 1
        elif self.use_patches_only:  # Exclude CLS token
            tokens_per_image = self.context_size - 1
        else:
            tokens_per_image = self.context_size

        return self.total_training_images * tokens_per_image

    @property
    def total_training_steps(self):
        """Calculates the total number of training steps based on token counts and batch size."""
        return self.total_training_tokens // self.train_batch_size

    @property
    def total_training_images(self):
        """Returns the total number of training images based on dataset and epochs."""
        # if self.dataset_name == "imagenet1k":
        dataset_size = 1_300_000
        # else:
        #     raise ValueError(
        #         "Your current dataset is not supported by the VisionModelSAERunnerConfig"
        #     )
        return int(dataset_size * self.num_epochs)

    @property
    def d_sae(self):
        """Calculates the SAE dimensionality based on input dimensions and expansion factor."""
        return self.d_in * self.expansion_factor

    @property
    def num_patch(self):
        """Calculates the number of patches based on the context size."""
        return int(math.sqrt(self.context_size - 1))

    def __post_init__(self):
        if self.b_dec_init_method not in ["geometric_median", "mean", "zeros"]:
            raise ValueError(
                f"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}"
            )

        if self.b_dec_init_method == "zeros":
            logging.warning(
                "Warning: We are initializing b_dec to zeros. This is probably not what you want."
            )

        if self.cls_token_only and self.use_patches_only:
            raise ValueError("cls_token_only and use_patches_only are exclusive.")

        # Autofill cached_activations_path unless the user overrode it
        # @TODO this here because I don't want to break backwards compability, but I think
        # we should refactor the activation cache fully
        if self.cached_activations_path is None:
            self.cached_activations_path = f"activations/{self.dataset_path.replace('/', '_')}/{self.model_name.replace('/', '_')}/{self.hook_point}"
            if self.hook_point_head_index is not None:
                self.cached_activations_path += f"_{self.hook_point_head_index}"

        if os.getenv("EVAL_MODE", "false").lower() in {"true", "1"}:
            self.is_training = False
            logging.info(
                "Evaluation mode detected via environment variable; setting is_training to False."
            )

        if self.cls_token_only and self.use_patches_only:
            raise ValueError(
                "Configuration error: 'cls_token_only' and 'use_patches_only' cannot both be True."
            )

        # Calculate key metrics
        n_tokens_per_buffer = (
            self.store_batch_size * self.context_size * self.n_batches_in_buffer
        )
        n_contexts_per_buffer = self.store_batch_size * self.n_batches_in_buffer
        n_feature_window_samples = (
            self.total_training_steps // self.feature_sampling_window
        )

        # fmt:off
        # Log useful information
        logging.info(f"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 1e6}")
        logging.info(
            f"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 1e6}"
        )
        logging.info(f"Total training steps: {self.total_training_steps}")
        logging.info(f"Total training images: {self.total_training_images}")
        logging.info(
            f"Total wandb updates: {self.total_training_steps // self.wandb_log_frequency}"
        )
        logging.info(f"Expansion factor: {self.expansion_factor}")

        logging.info(
            f"n_tokens_per_feature_sampling_window (millions): {self.feature_sampling_window * self.context_size * self.train_batch_size / 1e6}"
        )
        logging.info(
            f"n_tokens_per_dead_feature_window (millions): {self.dead_feature_window * self.context_size * self.train_batch_size / 1e6}"
        )
        logging.info(
            f"We will reset the sparsity calculation {n_feature_window_samples} times."
        )
        logging.info(
            f"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size:.2e}"
        )

        # Log configuration options
        if self.use_ghost_grads:
            logging.info("Using Ghost Grads.")

        if self.max_grad_norm:
            logging.info(f"Gradient clipping with max_norm={self.max_grad_norm}")

        logging.info(f"Using SAE initialization method: {self.initialization_method}")
        # fmt:on

    def is_property(self, attr_name):
        return isinstance(getattr(self.__class__, attr_name, None), property)

    def save_config(self, path: str):
        """
        Save the configuration to a JSON file at the given path.
        """
        # Convert the dataclass to a dictionary
        data = asdict(self)

        # Function to make data JSON-serializable
        def make_serializable(obj):
            if inspect.isdatadescriptor(obj):
                return

            if isinstance(obj, (list, tuple)):
                # Recursively process lists and tuples
                return [make_serializable(item) for item in obj]
            elif isinstance(obj, dict):
                # Recursively process dictionaries
                return {
                    key: make_serializable(value)
                    for key, value in obj.items()
                    if not self.is_property(key)
                }
            else:
                return obj  # Other types are left as-is

        # Process data to make it serializable
        serializable_data = make_serializable(data)
        serializable_data["_dtype"] = self._dtype
        serializable_data["_device"] = self._device

        # Save to JSON file
        with open(path, "w") as f:
            json.dump(serializable_data, f, indent=4)

    @classmethod
    def load_config(cls, path: str):
        """
        Load the configuration from a JSON file at the given path.
        """
        with open(path, "r") as f:
            data = json.load(f)

        # Function to reconstruct data types
        def reconstruct_types(obj):
            if isinstance(obj, dict):
                if "__type__" in obj:
                    # Reconstruct torch.device from string
                    return obj["value"]
                else:
                    # Recursively process dictionaries
                    return {key: reconstruct_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                # Recursively process lists
                return [reconstruct_types(item) for item in obj]
            else:
                return obj  # Other types are left as-is

        # Reconstruct data with proper types
        data = reconstruct_types(data)

        # Get the current dataclass fields
        current_fields = {f.name for f in fields(cls)}

        # Remove legacy fields that are not part of the dataclass anymore
        # For example, old configs might have "total_training_images" and "total_training_tokens"
        # which are now computed properties. We should remove them.
        for legacy_key in ["total_training_images", "total_training_tokens", "d_sae"]:
            if legacy_key in data:
                # Log a warning about the deprecated field being ignored
                logging.warning(
                    f"Deprecated field '{legacy_key}' found in config. It will be ignored."
                )
                # If you need to do something with these values, do it here before removing
                del data[legacy_key]

        # Filter out any other keys not defined as fields in the current class
        cleaned_data = {k: v for k, v in data.items() if k in current_fields}

        # Create an instance of the class with the reconstructed data
        return cls(**cleaned_data)

    def pretty_print(self):
        print("Configuration:")
        for field in fields(self):
            value = getattr(self, field.name)
            if isinstance(value, torch.dtype):
                value = str(value).split(".")[-1]  # Convert torch.dtype to string
            elif isinstance(value, torch.device):
                value = str(value)  # Convert torch.device to string
            print(f"  {field.name}: {value}")


@dataclass
class CacheActivationsRunnerConfig:
    """Configuration for caching activations of an LLM."""

    # Activation caching stuff
    shuffle_every_n_buffers: int = 10
    n_shuffles_with_last_section: int = 10
    n_shuffles_in_entire_dir: int = 10
    n_shuffles_final: int = 100

    def __post_init__(self):
        super().__post_init__()
        if self.use_cached_activations:
            raise ValueError(
                "Use_cached_activations should be False when running cache_activations_runner"
            )
