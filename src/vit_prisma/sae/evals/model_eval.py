from vit_prisma.sae.sae import SparseAutoencoder
from vit_prisma.sae.evals.evals import (get_substitution_loss, get_text_embeddings_openclip, 
                                        get_feature_probability)
from vit_prisma.models.base_vit import HookedViT
from vit_prisma.dataloaders.imagenet_index import imagenet_index
from vit_prisma.transforms.model_transforms import get_model_transforms
import matplotlib.pyplot as plt

import open_clip

import numpy as np
from statistics import mean
import torch
import torch.nn.functional as F
from torch.utils.data.dataloader import DataLoader

from tqdm import tqdm

import einops

import numpy as np
import torch.nn as nn
from torchvision.datasets import DatasetFolder
from torchvision.datasets.folder import default_loader

from vit_prisma.sae.evals.evals import zero_ablate_hook

from typing import Any, List, Tuple, Dict
from functools import partial


class LinearClassifier(nn.Module):
    """A simple linear layer on top of frozen features."""
    def __init__(self, dim, num_labels=1000):
        super(LinearClassifier, self).__init__()
        self.num_labels = num_labels
        self.linear = nn.Linear(dim, num_labels)
        self.linear.weight.data.normal_(mean=0.0, std=0.01)
        self.linear.bias.data.zero_()

    def forward(self, x):
        x = x.view(x.size(0), -1)
        return self.linear(x)


def load_pretrained_linear_weights(linear_classifier, model_name, patch_size):
    url = "dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth"
    if url is not None:
        print("Loading pretrained linear weights.")
        state_dict = torch.hub.load_state_dict_from_url(
            url="https://dl.fbaipublicfiles.com/dino/" + url
        )["state_dict"]
        new_state_dict = {
            'linear.weight': state_dict['module.linear.weight'],
            'linear.bias': state_dict['module.linear.bias']
        }
        linear_classifier.load_state_dict(new_state_dict, strict=True)
    else:
        print("Using random linear weights.")


class SparsecoderEval():

    def __init__(self, sc: SparseAutoencoder, model: HookedViT):
        
        self.sc = sc
        self.model = model
        self.is_transcoder = self.sc.cfg.is_transcoder

        self.hook_point_filters = [sc.cfg.hook_point]
        if self.is_transcoder:
            self.hook_point_filters.append(sc.cfg.out_hook_point)

        data_transforms = get_model_transforms(self.model.cfg.model_name)

        self.validation_dataset = DatasetFolder(
            root='/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val',
            loader=default_loader,
            extensions=('.jpg', '.jpeg', '.png'),
            transform=data_transforms
        )

        self.validation_dataloader = DataLoader(self.validation_dataset, batch_size=128, shuffle=True, num_workers=4)

        if 'dino' in self.model.cfg.model_name:
            self.is_dino = True
            self.classifier_head = LinearClassifier(1536, num_labels=1000).cuda()
            load_pretrained_linear_weights(self.classifier_head, 'vit', 16)
        else:
            self.is_dino = False


    def run_eval(self, is_clip=False):

        all_l0 = []
        all_l0_cls = []

        # image level l0
        all_l0_image = []

        total_loss = 0
        total_score = 0
        total_reconstruction_loss = 0
        total_zero_abl_loss = 0
        total_samples = 0
        all_cosine_similarity = []
        all_recons_cosine_similarity = []

        self.model.eval()
        self.sc.eval()

        if is_clip:
            num_imagenet_classes = 1000
            batch_label_names = [imagenet_index[str(int(label))][1] for label in range(num_imagenet_classes)]

            og_model, _, preproc = open_clip.create_model_and_transforms('hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K')
            tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K')

            text_embeddings = get_text_embeddings_openclip(og_model, preproc, tokenizer, batch_label_names)

        total_acts = None
        total_tokens = 0
        total_images = 0

        with torch.no_grad():
            pbar = tqdm(self.validation_dataloader, desc="Evaluating", dynamic_ncols=True)

            num_features = self.sc.W_enc.shape[1]

            total_patches = 0
            feature_activation_counts = torch.zeros(num_features, device='cuda')
            feature_activation_sums = torch.zeros(num_features, device='cuda')

            for _, batch in enumerate(pbar):
                batch_tokens, gt_labels = batch
                batch_tokens = batch_tokens.to(self.sc.device)
                batch_size = batch_tokens.shape[0]
                # batch shape
                total_samples += batch_size
                _, cache = self.model.run_with_cache(batch_tokens, names_filter=self.hook_point_filters)
                hook_point_activation = cache[self.hook_point_filters[0]].to(self.sc.device)

                if self.sc.cfg.use_patches_only:
                    hook_point_activation = hook_point_activation[:,1:,:]
                elif self.sc.cfg.cls_token_only:
                    hook_point_activation = hook_point_activation[:,0:1,:]
                
                args = [hook_point_activation]
                if self.is_transcoder:
                    out_hook_point_activation = cache[self.hook_point_filters[1]].to(self.sc.device)
                    args.append(out_hook_point_activation)

                sae_out, feature_acts, loss, mse_loss, l1_loss, _, aux_loss = self.sc(*args)


                batch_size, seq_len, _ = feature_acts.shape
                total_patches += batch_size * seq_len

                feature_acts_flat = feature_acts.reshape(-1, num_features)
                feature_active = (feature_acts_flat > 0).float()
                feature_activation_counts += feature_active.sum(dim=0)
                feature_activation_sums += feature_acts_flat.sum(dim=0)

                sae_activations = get_feature_probability(feature_acts)
                if total_acts is None:
                    total_acts = sae_activations.sum(0)
                else:
                    total_acts += sae_activations.sum(0)
                
                total_tokens += sae_activations.shape[0]
                total_images += batch_size

                l0 = (feature_acts > 0).float().sum(-1).detach()

                all_l0.extend(l0.mean(dim=1).cpu().numpy())
                l0_cls = (feature_acts[:, 0, :] > 0).float().sum(-1).detach()
                all_l0_cls.extend(l0_cls.flatten().cpu().numpy())

                l0 = (feature_acts > 0).float().sum(-1).detach()
                
                l0_per_sample = [
                    feature_acts[i].nonzero(as_tuple=True)[1].unique().numel()
                    for i in range(feature_acts.shape[0])
                ]
                all_l0_image.extend(l0_per_sample)

                cos_sim = torch.cosine_similarity(einops.rearrange(hook_point_activation, "batch seq d_mlp -> (batch seq) d_mlp"),
                                                                                einops.rearrange(sae_out, "batch seq d_mlp -> (batch seq) d_mlp"),
                                                                                    dim=0).mean(-1).tolist()
                all_cosine_similarity.append(cos_sim)

                if is_clip:
                    score, loss, recons_loss, zero_abl_loss = get_substitution_loss(self.sc, self.model, batch_tokens, gt_labels, 
                                                                            text_embeddings)
                elif self.is_dino:

                    class_logits = self.classifier_head(model(batch_tokens))

                    loss = F.cross_entropy(class_logits.cuda(), gt_labels.cuda())

                    head_index = self.sc.cfg.hook_point_head_index
                    hook_point = self.sc.cfg.out_hook_point if self.is_transcoder else self.sc.cfg.hook_point
                    
                    def standard_replacement_hook(activations: torch.Tensor, hook: Any):
                        activations = self.sc.forward(activations)[0].to(activations.dtype)
                        return activations

                    def head_replacement_hook(activations: torch.Tensor, hook: Any):
                        new_activations = self.sc.forward(activations[:, :, head_index])[0].to(activations.dtype)
                        activations[:, :, head_index] = new_activations
                        return activations

                    replacement_hook = standard_replacement_hook if head_index is None else head_replacement_hook
        
                    recons_image_embeddings = self.classifier_head(model.run_with_hooks(
                        batch_tokens,
                        fwd_hooks=[(hook_point, partial(replacement_hook))],
                    ))

                    recons_loss = F.cross_entropy(recons_image_embeddings.cuda(), gt_labels.cuda())

                    zero_abl_image_embeddings = self.classifier_head(model.run_with_hooks(
                        batch_tokens, fwd_hooks=[(hook_point, zero_ablate_hook)]
                    ))

                    zero_abl_loss = F.cross_entropy(zero_abl_image_embeddings.cuda(), gt_labels.cuda())

                    score = (zero_abl_loss - recons_loss) / (zero_abl_loss - loss)


                total_loss += loss.item()
                total_score += score.item()
                total_reconstruction_loss += recons_loss.item()
                total_zero_abl_loss += zero_abl_loss.item()
                # all_recons_cosine_similarity.extend(recons_cosine_sim)

                pbar.set_postfix({
                    'L0': f"{mean(l0_per_sample)}",
                    'Cosine Sim': f"{cos_sim:.6f}"
                })
                
        feature_activation_counts.to('cpu')
        feature_activation_sums.to('cpu')

        avg_loss = total_loss / len(self.validation_dataloader)
        avg_reconstruction_loss = total_reconstruction_loss / len(self.validation_dataloader)
        avg_zero_abl_loss = total_zero_abl_loss / len(self.validation_dataloader)
        avg_score = total_score / len(self.validation_dataloader)

        ce_recovered =  ((avg_zero_abl_loss - avg_reconstruction_loss) / ((avg_zero_abl_loss - avg_loss) + 1e-6)) * 100
        
        avg_l0 = np.mean(all_l0)
        avg_l0_cls = np.mean(all_l0_cls)
        avg_l0_image = np.mean(all_l0_image)

        avg_cos_sim = np.mean(all_cosine_similarity)
        # total_recons_cosine_similarity = np.mean(all_recons_cosine_similarity)

        metrics = {}
        metrics['avg_l0'] = avg_l0.astype(float)
        metrics['avg_cls_l0'] = avg_l0_cls.astype(float)
        metrics['avg_image_l0'] = avg_l0_image.astype(float)
        metrics['avg_cosine_similarity'] = avg_cos_sim.astype(float)
        # metrics['avg_recons_cosine_similarity'] = total_recons_cosine_similarity.astype(float)
        metrics['avg_CE'] = avg_loss
        metrics['avg_recons_CE'] = avg_reconstruction_loss
        metrics['avg_zero_abl_CE'] = avg_zero_abl_loss
        metrics['CE_recovered'] = ce_recovered

        # print out everything above
        print(f"Average L0 (features activated): {avg_l0:.6f}")
        print(f"Average L0 (features activated) per CLS token: {avg_l0_cls:.6f}")
        print(f"Average L0 (features activated) per image: {avg_l0_image:.6f}")
        print(f"Average Cosine Similarity: {avg_cos_sim:.4f}")
        print(f"Average Loss: {avg_loss:.6f}")
        print(f"Average Reconstruction Loss: {avg_reconstruction_loss:.6f}")
        print(f"Average Zero Ablation Loss: {avg_zero_abl_loss:.6f}")
        print(f"Average CE Score: {avg_score:.6f}")
        print(f"% CE recovered: {ce_recovered:.6f}")
        # print(f"Average Reconstruction Cosine Similarity: {total_recons_cosine_similarity:.6f}")
        

        return metrics
